{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "import torchvision as vs\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch import optim\n",
    "\n",
    "from os import listdir\n",
    "from os.path import splitext\n",
    "from glob import glob\n",
    "\n",
    "from PIL import Image\n",
    "from utils.nn_block import DualConv, DownConv, UpConv, OutputConv\n",
    "\n",
    "from torchsummary import summary\n",
    "import logging\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_channels, n_classes):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        \n",
    "        self.inp = DualConv(n_channels, 64)\n",
    "        \n",
    "        self.down_conv_1 = DownConv(64, 128)\n",
    "        self.down_conv_2 = DownConv(128, 256)\n",
    "        self.down_conv_3 = DownConv(256, 512)\n",
    "        self.down_conv_4 = DownConv(512, 1024)\n",
    "        \n",
    "        self.up_conv_1 = UpConv(1024, 512)\n",
    "        self.up_conv_2 = UpConv(512, 256)\n",
    "        self.up_conv_3 = UpConv(256, 128)\n",
    "        self.up_conv_4 = UpConv(128, 64)\n",
    "        \n",
    "        self.op_conv = OutputConv(64, n_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x1 = self.inp(x)\n",
    "        \n",
    "        x2 = self.down_conv_1(x1)\n",
    "        x3 = self.down_conv_2(x2)\n",
    "        x4 = self.down_conv_3(x3)\n",
    "        x5 = self.down_conv_4(x4)\n",
    "        \n",
    "        x6 = self.up_conv_1(x5, x4)\n",
    "        x7 = self.up_conv_2(x6, x3)\n",
    "        x8 = self.up_conv_3(x7, x2)\n",
    "        x9 = self.up_conv_4(x8, x1)\n",
    "        \n",
    "        result = self.op_conv(x9)\n",
    "        \n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicDataset(Dataset):\n",
    "    def __init__(self, imgs_dir, masks_dir, mask_suffix=''):\n",
    "        self.imgs_dir = imgs_dir\n",
    "        self.masks_dir = masks_dir\n",
    "        self.mask_suffix = mask_suffix\n",
    "\n",
    "        self.ids = [splitext(file)[0] for file in listdir(imgs_dir)\n",
    "                    if not file.startswith('.')]\n",
    "        logging.info(f'Creating dataset with {len(self.ids)} examples')\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "\n",
    "    @classmethod\n",
    "    def preprocess(cls, pil_img, newW, newH):\n",
    "        w, h = pil_img.size\n",
    "        \n",
    "        pil_img = pil_img.resize((newW, newH))\n",
    "\n",
    "        img_nd = np.array(pil_img)\n",
    "\n",
    "        if len(img_nd.shape) == 2:\n",
    "            img_nd = np.expand_dims(img_nd, axis=2)\n",
    "\n",
    "        # HWC to CHW\n",
    "        img_trans = img_nd.transpose((2, 0, 1))\n",
    "        if img_trans.max() > 1:\n",
    "            img_trans = img_trans / 255\n",
    "\n",
    "        return img_trans\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        idx = self.ids[i]\n",
    "        mask_file = glob(self.masks_dir + idx + self.mask_suffix + '.*')\n",
    "        img_file = glob(self.imgs_dir + idx + '.*')\n",
    "\n",
    "        assert len(mask_file) == 1, \\\n",
    "            f'Either no mask or multiple masks found for the ID {idx}: {mask_file}'\n",
    "        assert len(img_file) == 1, \\\n",
    "            f'Either no image or multiple images found for the ID {idx}: {img_file}'\n",
    "        mask = Image.open(mask_file[0])\n",
    "        img = Image.open(img_file[0])\n",
    "\n",
    "        assert img.size == mask.size, \\\n",
    "            f'Image and mask {idx} should be the same size, but are {img.size} and {mask.size}'\n",
    "\n",
    "        img = self.preprocess(img, 360, 480)\n",
    "        mask = self.preprocess(mask, 360, 480)\n",
    "\n",
    "        return {\n",
    "            'image': torch.from_numpy(img).type(torch.FloatTensor),\n",
    "            'mask': torch.from_numpy(mask).type(torch.FloatTensor)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGES_PATH = '/data/Data/midv500_data/dataset/images/'\n",
    "MASKS_PATH = '/data/Data/midv500_data/dataset/masks/'\n",
    "MODEL_CHECKPOINT_PATH = '/data/Data/midv500_data/dataset/checkpoints/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = BasicDataset(IMAGES_PATH, MASKS_PATH, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, device, img_dir, mask_dir, epochs=5, lr=0.001, val_split=0.20, batch_size=1):\n",
    "    dataset = BasicDataset(img_dir, mask_dir)\n",
    "    val_samples = int(len(dataset) * val_split)\n",
    "    train_samples = len(dataset) - val_samples\n",
    "    train, val = random_split(dataset, [train_samples, val_samples])\n",
    "    train_loader = DataLoader(train, batch_size=batch_size, shuffle=True, num_workers=8)\n",
    "    val_loader = DataLoader(val, batch_size=batch_size, shuffle=False, num_workers=8, drop_last=True)\n",
    "    \n",
    "    writer = SummaryWriter(comment=f'LR_{lr}_BS_{batch_size}')\n",
    "    global_step = 0\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-8)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        \n",
    "        losses = []\n",
    "        val_losses = []\n",
    "        avg_val_loss = np.inf\n",
    "        with tqdm(total=train_samples, desc=f'Epoch {epoch + 1}/{epochs}', unit='img') as pbar:\n",
    "            for batch in train_loader:\n",
    "                imgs = batch['image']\n",
    "                true_masks = batch['mask']\n",
    "                assert imgs.shape[1] == model.n_channels, \\\n",
    "                    f'Network has been defined with {model.n_channels} input channels, ' \\\n",
    "                    f'but loaded images have {imgs.shape[1]} channels. Please check that ' \\\n",
    "                    'the images are loaded correctly.'\n",
    "                \n",
    "                imgs = imgs.to(device=device, dtype=torch.float32)\n",
    "                mask_type = torch.float32 if model.n_classes == 1 else torch.long\n",
    "                true_masks = true_masks.to(device=device, dtype=mask_type)\n",
    "                \n",
    "                masks_pred = model(imgs)\n",
    "                loss = criterion(masks_pred, true_masks)\n",
    "                losses.append(loss.item())\n",
    "                writer.add_scalar('Loss/train', sum(losses)/len(losses), global_step)\n",
    "\n",
    "                pbar.set_postfix(**{'loss: ': sum(losses)/len(losses)})\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                nn.utils.clip_grad_value_(model.parameters(), 0.1)\n",
    "                optimizer.step()\n",
    "                \n",
    "                pbar.update(imgs.shape[0])\n",
    "                global_step += 1\n",
    "\n",
    "            val_loss = 0\n",
    "            for val_batch in val_loader:\n",
    "                imgs, true_masks = val_batch['image'], val_batch['mask']\n",
    "                imgs = imgs.to(device=device, dtype=torch.float32)\n",
    "                true_masks = true_masks.to(device=device, dtype=torch.float32)\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    mask_pred = model(imgs)\n",
    "                \n",
    "                pred = torch.sigmoid(mask_pred)\n",
    "                pred = (pred > 0.5).float()\n",
    "                val_loss += criterion(masks_pred, true_masks)\n",
    "            val_score = val_loss / len(val_loader)\n",
    "            val_losses.append(val_score)\n",
    "            avg_val_loss = sum(val_losses) / len(val_losses)\n",
    "            pbar.set_postfix(**{'loss: ': sum(losses)/len(losses), 'val_loss: ': avg_val_loss})\n",
    "        \n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "unet = UNet(n_channels=3, n_classes=1)\n",
    "# summary(unet.cuda(), (3, 480, 360))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Device:\", device)\n",
    "unet = unet.to(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(unet,\n",
    "          device,\n",
    "          IMAGES_PATH,\n",
    "          MASKS_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
